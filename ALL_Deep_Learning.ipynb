{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALL_Deep_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1APbMlMNE0t7-S9ocA5NiqeQEtnQmehp5",
      "authorship_tag": "ABX9TyNvcGL8LwkLgWFuPRhgM9B6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anonymous1846/ALL-Detection-Using-CNN/blob/master/ALL_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz8z3s6bmZ-L"
      },
      "source": [
        "# **Importing All The Necessary Dependancies.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7XLGtoAl0zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62b7b58-bd60-4d35-88fa-debc088cb74c"
      },
      "source": [
        "#importing all the required dependencies \n",
        "#keras act as an interface for python ai(Tensorflow libs)\n",
        "import keras\n",
        "#validating the keras installation !\n",
        "#The problem with Deep Learning is that we require a lot of data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#import matlabplotlib for plotting purposes !\n",
        "import matplotlib.pyplot as plt\n",
        "'''for building the cnn model for binary and multiple classification !\n",
        "The layers for multiple layers are also imported'''\n",
        "import os \n",
        "from keras.models import Sequential\n",
        "from keras.layers import MaxPooling2D,Conv2D,Dropout,Dense,Flatten,BatchNormalization,MaxPool2D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import random\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "print(keras.__version__)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JFZFFZFzII"
      },
      "source": [
        "# **Loading the Dataset(i.e Cancer cell images).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNvpvCN5TJqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d373574-7cb2-40dc-c7fc-3dff039b7171"
      },
      "source": [
        "#the path to the cancer and normal datasets\n",
        "training_data_path=\"/content/drive/MyDrive/Architecture Datasets/Training\"\n",
        "print(f'The training images loaded from path :{training_data_path}')\n",
        "validation_data_path=\"/content/drive/MyDrive/Architecture Datasets/Validation\"\n",
        "print(f'The validation images loaded from path :{validation_data_path}')\n",
        "#printing the number of cancer and normal data cancer cells\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training images loaded from path :/content/drive/MyDrive/Architecture Datasets/Training\n",
            "The validation images loaded from path :/content/drive/MyDrive/Architecture Datasets/Validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yT7JaUnVPpU"
      },
      "source": [
        "# **Image Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPeiTL-oVTay"
      },
      "source": [
        "'''The image dataset contains roughly 334 images, but the main problem is that it is insufficient \n",
        "for deep learning training, so inorder to rectify that we use the ImageDataGenerator to generate \n",
        "images from the pre-exisiting images ! the below function will genearte the new images based on the paramters\n",
        "provided to that function'''\n",
        "training_images_preprocessed=ImageDataGenerator(rescale=1./255,rotation_range=40)\n",
        "validate_images_preprocessed=ImageDataGenerator(rescale=1./255,rotation_range=40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAop9bDXLv9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGPm_71JXPeC"
      },
      "source": [
        "# **Applying the changes to the images from the directory !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9G6VPiSXVsR",
        "outputId": "f7b4c241-d587-4e53-e551-2234c084d836"
      },
      "source": [
        "training_images_datagen=training_images_preprocessed.flow_from_directory(\n",
        "        training_data_path,  \n",
        "        target_size=(300, 300),  \n",
        "        batch_size=128,\n",
        "        class_mode='binary')\n",
        "print(training_images_datagen.class_indices)\n",
        "\n",
        "\n",
        "valid_images_datagen=validate_images_preprocessed.flow_from_directory(\n",
        "        validation_data_path,  \n",
        "        target_size=(300, 300),  \n",
        "        batch_size=128,\n",
        "        class_mode='binary')\n",
        "print(valid_images_datagen.class_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 268 images belonging to 2 classes.\n",
            "{'Blasts': 0, 'Normal': 1}\n",
            "Found 33 images belonging to 2 classes.\n",
            "{'Blasts': 0, 'Normal': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psRQ77sbYuqN"
      },
      "source": [
        "# **Representation of Images.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3oV8CEwY6p8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "92894283-2651-4ddc-a0c4-ed07ea98ab1e"
      },
      "source": [
        "#plotting the images ! After the augemntation !\n",
        "def plot(image):\n",
        "  fig,axes=plt.subplots(1,5,figsize=(20,20))\n",
        "  axes=axes.flatten()\n",
        "  for imag,axis in zip(image,axes):\n",
        "    #the zip function takes two params(iteratables and combines then to form a list of tuples !)\n",
        "    axis.imshow(imag)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "training_images_datagen.class_indices\n",
        "sample_images=[training_images_datagen[0][0][0] for i in range(5)]\n",
        "plot(sample_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-27bacc98f466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtraining_images_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msample_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_images_datagen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-27bacc98f466>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtraining_images_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msample_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_images_datagen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[1;32m     64\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExfBhm4YE0f"
      },
      "source": [
        "# **Building the CNN Deep Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66QFUFr1YNlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf2d265-6ab6-4bd9-b9d8-3a651c655a56"
      },
      "source": [
        "'''the classification model output will e categorical !\n",
        "list of the deep learning layers added to the model for training !\n",
        "five layers for the model'''\n",
        "the_binary_cnn_model=Sequential([\n",
        "                                    Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "                                    MaxPooling2D(2, 2),\n",
        "                                    # The second convolution\n",
        "                                    Conv2D(32, (3,3), activation='relu'),\n",
        "                                    MaxPooling2D(2,2),\n",
        "                                    # The third convolution\n",
        "                                    Conv2D(64, (3,3), activation='relu'),\n",
        "                                    MaxPooling2D(2,2),\n",
        "                                    # The fourth convolution\n",
        "                                    Conv2D(64, (3,3), activation='relu'),\n",
        "                                    MaxPooling2D(2,2),\n",
        "                                    # The fifth convolution\n",
        "                                    Conv2D(64, (3,3), activation='relu'),\n",
        "                                    MaxPooling2D(2,2),\n",
        "                                    # Flatten the  results to feed into a DNN\n",
        "                                    Flatten(),\n",
        "                                    # 512 neuron hidden layer\n",
        "                                    Dense(512, activation='relu'),\n",
        "                                    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('normal') and 0 for the other 'cancer'\n",
        "                                    Dense(1, activation='sigmoid')\n",
        "                                        ])\n",
        "\n",
        "print(the_binary_cnn_model)\n",
        "\n",
        "# the_binary_cnn_model = keras.models.Sequential([\n",
        "#     Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(300, 300, 3)),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "#     Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "#     Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "#     BatchNormalization(),\n",
        "#     Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "#     BatchNormalization(),\n",
        "#     Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "#     Flatten(),\n",
        "#     Dense(4096, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(4096, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(1, activation='softmax')\n",
        "# ])\n",
        "# print(the_binary_cnn_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f51b90b46d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN0CNdCST2ma"
      },
      "source": [
        "# **Model Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xhxnXl_T577"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvVW4YgZT82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54673f29-6717-4663-eb1f-faa9680b6354"
      },
      "source": [
        "the_binary_cnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVvlM0Zt59cN"
      },
      "source": [
        "# **Compile the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5dFUoZ46AMX"
      },
      "source": [
        "#applying the optimizers\n",
        "#judging based on the accuracy !\n",
        "the_binary_cnn_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UovyYQST6aey"
      },
      "source": [
        "# **Training the model based on the above model !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IASzW7LI6e7N",
        "outputId": "e08a40e4-ce31-400d-9c15-1eca33d343e1"
      },
      "source": [
        "#the steps per epoch is set to 2, but cannot be set to more than 3(because of error !)\n",
        "the_final_train = the_binary_cnn_model.fit(\n",
        "\n",
        "                      training_images_datagen,\n",
        "                      batch_size=10,\n",
        "                      validation_data=valid_images_datagen,\n",
        "                      steps_per_epoch=3,  \n",
        "                      epochs=100,\n",
        "                      verbose=1,\n",
        "                    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - 13s 6s/step - loss: 0.6012 - accuracy: 0.8321 - val_loss: 1.2678 - val_accuracy: 0.5455\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.6700 - accuracy: 0.7985 - val_loss: 0.4470 - val_accuracy: 0.7576\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.3323 - accuracy: 0.8881 - val_loss: 0.3760 - val_accuracy: 0.8182\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.2956 - accuracy: 0.8769 - val_loss: 0.3823 - val_accuracy: 0.8182\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.2002 - accuracy: 0.9179 - val_loss: 0.2581 - val_accuracy: 0.8788\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1344 - accuracy: 0.9552 - val_loss: 0.1772 - val_accuracy: 0.9394\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1170 - accuracy: 0.9627 - val_loss: 0.2268 - val_accuracy: 0.9091\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0793 - accuracy: 0.9776 - val_loss: 0.1707 - val_accuracy: 0.9091\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0743 - accuracy: 0.9664 - val_loss: 0.2389 - val_accuracy: 0.9091\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.4714 - accuracy: 0.8060 - val_loss: 0.3747 - val_accuracy: 0.8788\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.2270 - accuracy: 0.9104 - val_loss: 0.3531 - val_accuracy: 0.8485\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0975 - accuracy: 0.9664 - val_loss: 0.2889 - val_accuracy: 0.9091\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0765 - accuracy: 0.9813 - val_loss: 0.3908 - val_accuracy: 0.8485\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0665 - accuracy: 0.9851 - val_loss: 0.3196 - val_accuracy: 0.8788\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1356 - accuracy: 0.9403 - val_loss: 0.3436 - val_accuracy: 0.8788\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0335 - accuracy: 0.9925 - val_loss: 0.3355 - val_accuracy: 0.8788\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0365 - accuracy: 0.9851 - val_loss: 0.3059 - val_accuracy: 0.9697\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0711 - accuracy: 0.9627 - val_loss: 0.9176 - val_accuracy: 0.7273\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.3318 - accuracy: 0.8881 - val_loss: 0.3251 - val_accuracy: 0.8485\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1482 - accuracy: 0.9403 - val_loss: 0.4594 - val_accuracy: 0.8788\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0738 - accuracy: 0.9776 - val_loss: 0.4100 - val_accuracy: 0.9091\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0262 - accuracy: 0.9925 - val_loss: 0.4249 - val_accuracy: 0.8788\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0285 - accuracy: 0.9888 - val_loss: 0.5930 - val_accuracy: 0.9394\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.5851 - accuracy: 0.8172 - val_loss: 0.4159 - val_accuracy: 0.8182\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1060 - accuracy: 0.9664 - val_loss: 0.3580 - val_accuracy: 0.8485\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0563 - accuracy: 0.9813 - val_loss: 0.3805 - val_accuracy: 0.8485\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0392 - accuracy: 0.9888 - val_loss: 0.3965 - val_accuracy: 0.9091\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0455 - accuracy: 0.9888 - val_loss: 0.2759 - val_accuracy: 0.9091\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0499 - accuracy: 0.9925 - val_loss: 0.5358 - val_accuracy: 0.9394\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.1562 - accuracy: 0.9552 - val_loss: 0.3900 - val_accuracy: 0.8182\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0344 - accuracy: 0.9851 - val_loss: 0.2961 - val_accuracy: 0.8788\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0169 - accuracy: 0.9963 - val_loss: 0.3956 - val_accuracy: 0.8788\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0201 - accuracy: 0.9925 - val_loss: 0.2148 - val_accuracy: 0.9091\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1536 - accuracy: 0.9552 - val_loss: 0.2028 - val_accuracy: 0.9091\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1596 - accuracy: 0.9552 - val_loss: 0.3727 - val_accuracy: 0.8485\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0576 - accuracy: 0.9701 - val_loss: 0.2287 - val_accuracy: 0.9091\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.0487 - accuracy: 0.9776 - val_loss: 0.4777 - val_accuracy: 0.8485\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0258 - accuracy: 0.9925 - val_loss: 0.3114 - val_accuracy: 0.9091\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0895 - accuracy: 0.9739 - val_loss: 0.3737 - val_accuracy: 0.9091\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - 11s 6s/step - loss: 0.6226 - accuracy: 0.8507 - val_loss: 0.6601 - val_accuracy: 0.8182\n",
            "Epoch 41/100\n",
            "3/3 [==============================] - 12s 4s/step - loss: 0.2687 - accuracy: 0.8993 - val_loss: 0.2925 - val_accuracy: 0.9091\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0725 - accuracy: 0.9776 - val_loss: 0.1537 - val_accuracy: 0.8788\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0498 - accuracy: 0.9888 - val_loss: 0.1636 - val_accuracy: 0.9091\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0308 - accuracy: 0.9963 - val_loss: 0.2244 - val_accuracy: 0.9091\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0271 - accuracy: 0.9925 - val_loss: 0.2292 - val_accuracy: 0.9091\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.0739 - accuracy: 0.9813 - val_loss: 0.4415 - val_accuracy: 0.8788\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0450 - accuracy: 0.9888 - val_loss: 0.3915 - val_accuracy: 0.9091\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.2203 - accuracy: 0.9291 - val_loss: 0.3881 - val_accuracy: 0.8788\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 6.5857 - accuracy: 0.6493 - val_loss: 1.4531 - val_accuracy: 0.7879\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.1693 - accuracy: 0.9366 - val_loss: 0.7642 - val_accuracy: 0.8788\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.1051 - accuracy: 0.9664 - val_loss: 0.6017 - val_accuracy: 0.8788\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.1089 - accuracy: 0.9627 - val_loss: 0.6399 - val_accuracy: 0.8485\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0486 - accuracy: 0.9925 - val_loss: 0.6641 - val_accuracy: 0.8485\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0433 - accuracy: 0.9888 - val_loss: 0.6015 - val_accuracy: 0.8788\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0317 - accuracy: 0.9925 - val_loss: 0.5869 - val_accuracy: 0.8485\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0250 - accuracy: 0.9963 - val_loss: 0.6837 - val_accuracy: 0.8485\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0190 - accuracy: 0.9963 - val_loss: 0.6351 - val_accuracy: 0.8788\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.5512 - val_accuracy: 0.8788\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0278 - accuracy: 0.9963 - val_loss: 0.6259 - val_accuracy: 0.8788\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0174 - accuracy: 0.9925 - val_loss: 0.5757 - val_accuracy: 0.8788\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0177 - accuracy: 0.9925 - val_loss: 0.6390 - val_accuracy: 0.8788\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.4762 - val_accuracy: 0.8485\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.2942 - accuracy: 0.9142 - val_loss: 0.4511 - val_accuracy: 0.9091\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 1.3376 - accuracy: 0.8545 - val_loss: 1.2517 - val_accuracy: 0.6061\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.4843 - accuracy: 0.8134 - val_loss: 0.3680 - val_accuracy: 0.8788\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0449 - accuracy: 0.9851 - val_loss: 0.3462 - val_accuracy: 0.8788\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.8485\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0322 - accuracy: 0.9963 - val_loss: 0.4939 - val_accuracy: 0.8788\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0414 - accuracy: 0.9851 - val_loss: 0.3983 - val_accuracy: 0.8788\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0208 - accuracy: 0.9963 - val_loss: 0.4992 - val_accuracy: 0.8788\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.4039 - val_accuracy: 0.8788\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.5273 - val_accuracy: 0.8788\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0121 - accuracy: 0.9925 - val_loss: 0.5152 - val_accuracy: 0.8485\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.8788\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - 11s 4s/step - loss: 0.0053 - accuracy: 0.9963 - val_loss: 0.4221 - val_accuracy: 0.8788\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.8788\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5702 - val_accuracy: 0.8788\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0615 - accuracy: 0.9776 - val_loss: 0.5762 - val_accuracy: 0.8485\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 1.1911 - accuracy: 0.8060 - val_loss: 0.3149 - val_accuracy: 0.8788\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0499 - accuracy: 0.9963 - val_loss: 0.1962 - val_accuracy: 0.9091\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.8788\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.3752 - val_accuracy: 0.8788\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.4523 - val_accuracy: 0.9091\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9091\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2431 - val_accuracy: 0.9091\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.5648 - val_accuracy: 0.8788\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5325 - val_accuracy: 0.8788\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0187 - accuracy: 0.9925 - val_loss: 0.5631 - val_accuracy: 0.8788\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.5576 - accuracy: 0.9142 - val_loss: 8.6052 - val_accuracy: 0.5455\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.7282 - accuracy: 0.8843 - val_loss: 0.8440 - val_accuracy: 0.8788\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.7568 - val_accuracy: 0.8788\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.9018 - val_accuracy: 0.8485\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.8967 - val_accuracy: 0.8485\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0079 - accuracy: 0.9963 - val_loss: 0.7551 - val_accuracy: 0.8788\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.8940 - val_accuracy: 0.8788\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4966 - val_accuracy: 0.8788\n",
            "Epoch 97/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.9852 - val_accuracy: 0.8788\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - 11s 3s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.8788\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0101 - accuracy: 0.9963 - val_loss: 0.6944 - val_accuracy: 0.8788\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - 11s 5s/step - loss: 0.0206 - accuracy: 0.9888 - val_loss: 0.7816 - val_accuracy: 0.8788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKqlawEa-J0E"
      },
      "source": [
        "# **Samaple Test and Validation Images !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8dEYa0P-JQY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsPLPfYqXlJ1"
      },
      "source": [
        "#**Testing for Cancer vs Normal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNA2kLHveNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f617274-894e-46ec-ab2c-86008a8cfbb6"
      },
      "source": [
        "def check_for_cancer(image_path):\n",
        "  img = image.load_img(image_path, target_size=(300, 300))\n",
        "  img = image.img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  img = np.vstack([img])\n",
        "  classes = the_binary_cnn_model.predict(img,batch_size=10).round(3)\n",
        "  #pred=np.argmax(classes)\n",
        "  if classes<0.5:\n",
        "      return \"Cancer cell\"\n",
        "  else:\n",
        "      return \"Normal cell\"\n",
        "#the five conditions for checking the cancer and normal cells !\n",
        "\n",
        "#checking for cancer/not !\n",
        "#cancerous !\n",
        "# Load image \n",
        "print(check_for_cancer('/content/drive/MyDrive/Architecture Datasets/Testing/Blasts/Im001_1.jpg'))\n",
        "print(check_for_cancer('/content/drive/MyDrive/Architecture Datasets/Validation/Normal/Im035_0.jpg'))\n",
        "print(check_for_cancer('/content/drive/MyDrive/Architecture Datasets/Testing/Blasts/Im001_1.tif'))\n",
        "print(check_for_cancer('/content/drive/MyDrive/Architecture Datasets/Validation/Normal/Im035_0.jpg'))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cancer cell\n",
            "Normal cell\n",
            "Cancer cell\n",
            "Normal cell\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}